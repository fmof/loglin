\documentclass[11pt]{article}
\usepackage[colorlinks]{hyperref}
\usepackage{fullpage}
\usepackage{enumitem}
\usepackage[noend]{algpseudocode}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{xspace}
\newcommand{\defeq}{\;\stackrel{\mbox{\tiny def}}{=}\;}
\newcommand{\horizrule}{\noindent\rule{\textwidth}{2pt}}
\newcommand{\vtheta}{\vec{\theta}}
\newcommand{\ph}{\hat{p}}
\newcommand{\pdisc}{p_{\rm disc}}
\newcommand{\Real}{{\mathbb R}}
\newcommand{\diffk}{\frac{\partial}{\partial \theta_k}}

\begin{document}
\title{\vspace{-2cm}Log-Linear Models}
\author{Jason Eisner (Johns Hopkins University)}
\date{}
\maketitle

\begin{center}
\framebox{\parbox{4.75in}{This handout is intended to be used with the
    interactive visualization at \\
  \hspace*{\fill} \url{http://cs.jhu.edu/~jason/tutorials/loglin/}. \hspace*{\fill}}}
\end{center}

\section{Starting Out Concretely}\label{sec:concrete}

\subsection{Formulas for Lesson 1}

The four shape probabilities in lesson 1 are computed as follows,
where $\theta_{\textrm{circle}}$ and $\theta_{\textrm{solid}}$ are the
two parameters that you can control with the sliders.

% Note: My typesetting with \hspace here is awfully hacky.
% I should really define boxes for the summands and use the width of the boxes.

\begin{align}
p(\text{solid circle}) & \defeq \frac{\exp (\theta_{\textrm{circle}}+\theta_{\textrm{solid}})\hspace{57mm}}{\exp (\theta_{\textrm{circle}} + \theta_{\textrm{solid}}) + \exp (\theta_{\textrm{circle}}) + \exp (\theta_{\textrm{solid}}) + \exp 0} \label{eqn:solidcircle}\\
\rule{0in}{1.5\baselineskip}
p(\text{striped circle}) & \defeq \frac{\exp (\theta_{\textrm{circle}})\hspace{0mm}}{\exp (\theta_{\textrm{circle}} + \theta_{\textrm{solid}}) + \exp (\theta_{\textrm{circle}}) + \exp (\theta_{\textrm{solid}}) + \exp 0} \label{eqn:stripedcircle}\\
\rule{0in}{1.5\baselineskip}
p(\text{solid triangle}) & \defeq \frac{\hspace{44mm}\exp (\theta_{\textrm{solid}})}{\exp (\theta_{\textrm{circle}} + \theta_{\textrm{solid}}) + \exp (\theta_{\textrm{circle}}) + \exp (\theta_{\textrm{solid}}) + \exp 0} \label{eqn:solidtriangle}\\
\rule{0in}{1.5\baselineskip}
p(\text{striped triangle}) & \defeq \frac{\hspace{79mm}\exp 0}{\exp (\theta_{\textrm{circle}} + \theta_{\textrm{solid}}) + \exp (\theta_{\textrm{circle}}) + \exp (\theta_{\textrm{solid}}) + \exp 0} \label{eqn:stripedtriangle}
\end{align}

``$\exp$'' denotes the exponential function.  Recall that $\exp x$ means
$e^x$ where $e$ is a constant (namely $e=2.71828\ldots$, the base of
natural logarithms).\footnote{If $y=e^x$, then $x=\log y$.  That's the
  definition of $\log y$ (sometimes written as $\log_e y$ or $\ln y$),
  the natural logarithm.}  From the graph of $y=e^x$ you can see:

\includegraphics[height=2.25in]{exp}
\begin{minipage}[b]{4in}
\vspace{4pt}
\begin{itemize}

\item $\exp x$ is always positive (except that $\exp
  -\infty=0$).  So all of the numerators and denominators in \eqref{eqn:solidcircle}--\eqref{eqn:stripedtriangle} are
  $\geq 0$.  This makes all the probabilities $\geq 0$, as required.

\item As $x$ gets bigger, so does $\exp x$.  So raising
  $\theta_{\textrm{solid}}$ with your slider will raise the numerators
  of $p(\text{solid circle})$ \eqref{eqn:solidcircle} and $p(\text{solid triangle})$
  \eqref{eqn:solidtriangle}.  To be precise, each time you increase
  $\theta_{\textrm{solid}}$ by 1, you multiply those numerators by
  $e$.  (And decreasing $\theta_{\textrm{solid}}$ by 1 divides them by $e$.)

\item Finally, the four probabilities
  \eqref{eqn:solidcircle}--\eqref{eqn:stripedtriangle} always sum to 1
  (thanks to their common denominator $Z$).  That's required because
  those are the four possible outcomes:
\end{itemize}
\end{minipage}
\vspace{-6pt}
\begin{equation}
  p(\text{solid circle}) + p(\text{striped circle}) + p(\text{solid circle}) + p(\text{striped triangle}) = 1
\end{equation}
Hence, when raising $\theta_{\textrm{solid}}$ makes the solid
probabilities \eqref{eqn:solidcircle} and \eqref{eqn:solidtriangle} go
up, the striped probabilities \eqref{eqn:stripedcircle} and
\eqref{eqn:stripedtriangle} must go down to compensate.  \hspace{-2pt}This keeps the sum at 1.  Lowering $\theta_{\textrm{solid}}$ does the reverse.

\subsection{Formulas for Lesson 2}

Now every shape has exactly two features (whereas in lesson 1, some shapes had fewer):

\begin{align*}
\hspace{-12mm}p(\text{solid circle}) & \defeq \frac{\exp (\theta_{\textrm{circle}}+\theta_{\textrm{solid}})\hspace{118mm}}{\exp (\theta_{\textrm{circle}} + \theta_{\textrm{solid}}) + \exp (\theta_{\textrm{circle}} + \theta_{\textrm{striped}}) + \exp (\theta_{\textrm{triangle}} + \theta_{\textrm{solid}}) + \exp (\theta_{\textrm{triangle}} + \theta_{\textrm{striped}})} \\
\rule{0in}{1.5\baselineskip}
\hspace{-12mm}p(\text{striped circle}) & \defeq \frac{\exp (\theta_{\textrm{circle}}+\theta_{\textrm{striped}})\hspace{44mm}}{\exp (\theta_{\textrm{circle}} + \theta_{\textrm{solid}}) + \exp (\theta_{\textrm{circle}} + \theta_{\textrm{striped}}) + \exp (\theta_{\textrm{triangle}} + \theta_{\textrm{solid}}) + \exp (\theta_{\textrm{triangle}} + \theta_{\textrm{striped}})} \\
\rule{0in}{1.5\baselineskip}
\hspace{-12mm}p(\text{solid triangle}) & \defeq \frac{\hspace{32mm}\exp (\theta_{\textrm{triangle}}+\theta_{\textrm{solid}})}{\exp (\theta_{\textrm{circle}} + \theta_{\textrm{solid}}) + \exp (\theta_{\textrm{circle}} + \theta_{\textrm{striped}}) + \exp (\theta_{\textrm{triangle}} + \theta_{\textrm{solid}}) + \exp (\theta_{\textrm{triangle}} + \theta_{\textrm{striped}})} \\
\rule{0in}{1.5\baselineskip}
\hspace{-12mm}p(\text{striped triangle}) & \defeq \frac{\hspace{111mm}\exp (\theta_{\textrm{triangle}}+\theta_{\textrm{striped}})}{\exp (\theta_{\textrm{circle}} + \theta_{\textrm{solid}}) + \exp (\theta_{\textrm{circle}} + \theta_{\textrm{striped}}) + \exp (\theta_{\textrm{triangle}} + \theta_{\textrm{solid}}) + \exp (\theta_{\textrm{triangle}} + \theta_{\textrm{striped}})} 
\end{align*}

\section{Why We Need Log-Linear Models {\em [read after lesson 2]}}

{\bf Note:} Lessons 1--10 will let you experiment with simple
probabilities $p(y)$.  Lesson 11 will finally move to the more general
case of conditional probabilities $p(y\mid x)$.  This handout will
{\em focus} on conditional probabilities, mainly in order to avoid saying
everything twice.  If you want to think first about the case of simple
probabilities, just ignore $x$ wherever it appears; also see
section~\ref{sec:unconditioned}.

\subsection{Modeling probabilities}

We often want to model conditional probabilities like $p(y
\mid x)$, where $y$ and $x$ are discrete outcomes.
\begin{itemize}[noitemsep]
\item They can be used to predict $y$ directly, in a context $x$.
\item We can build a more complicated model as a product
  of such probabilities.\footnote{E.g., Bayes' theorem, Bayesian
    networks, (hidden) Markov models, probabilistic context-free
    grammars, \ldots}
\end{itemize}
But where do we get these probabilities?

\subsection{Notation}

Suppose $X$ denotes a random context and $Y$ is a random event that
happens in that context.  Let $\ph(Y=y \mid X=x)$ be our estimate of
the conditional probability that the event will turn out to be $y$ when
the context happens to be $x$.  We'll abbreviate this as
$\ph(y\mid x)$.  The little ``hat'' over the $p$ is a reminder that
this is just an estimate, not the true probability.

\subsection{Simple methods}\label{sec:mle}

A simple idea is to estimate each conditional probability separately,
using simple count ratios: 
\begin{equation}
\ph(y \mid x) \defeq \frac{\textrm{count}(x,y)}{\textrm{count}(x)}
\end{equation}

Such a ``maximum-likelihood'' estimate is {\em unbiased} (correct on
average, when estimated from a {\em random} dataset).  But unfortunately, it
may have {\em high variance} (it is sensitive to the {\em particular}
dataset from which you obtain the counts).  It will be unreliable if
either count is small:
\begin{itemize}[noitemsep]
\item Suppose the denominator $\textrm{count}(x)$ is 2, then the estimate
  will be either 0, 0.5, or 1.  These estimates are quite different,
  and which one you get depends on your particular dataset.  
\item It's even worse if the denominator is 1 or 0.  What happens in
  each case?
\item If the numerator $\textrm{count}(x,y)$ is 0, meaning that $y$
  was not {\em observed} in context $x$, then the estimate will
  be 0, indicating that $y$ is {\em impossible} in context $x$.  
\item If the numerator is 1, then the estimate might be too high,
  since perhaps $y$ is just one of many unlikely events---it's the one
  we happened to see.
\end{itemize}

\noindent
``Smoothing'' techniques try to reduce the variance of this estimate.
However, they still estimate each conditional probability more or less
independently of the others.\footnote{Though ``backoff smoothing'' does jointly
  estimate $p(y \mid x_1)$ and $p(y \mid x_2)$ if $x_1, x_2$ are
  related in a certain way.}
% so that $\ph(y \mid x_1) \approx \ph(y \mid x_2)$ if the contexts
% $x_1 \approx x_2$ are similar and have small counts,
%  but not in a very flexible way.

\section{Log-Linear Models {\em [read after lesson 2]}}

Log-linear modeling is a very popular and flexible technique for
addressing this problem.  It has the advantage that it considers {\em
  descriptions} of the events.  Contextualized events $(x,y)$ with
{\em similar} descriptions tend to have similar probabilities---a form
of generalization.
% That is, if $\vec{f}(x_1,y_1) \approx \vec{f}(x_2,y_2)$, then
% $\hat{p}(y_1 \mid x_1) \approx \hat{p}(y_2 \mid x_2)$.  

\subsection{Feature functions}\label{sec:featfuncs}

Since you are the one who knows something about your problem, {\em
  you} get to define the function $\vec{f}$ that extracts these
descriptions.  If you decide that your model will have $K$ features,
then $\vec{f}(x,y)$ denotes a vector of $K$ real numbers $(f_1(x,y),
f_2(x,y), \ldots, f_K(x,y))$ that {\em describe} event $y$ in context
$x$.  These $K$ numbers are called the {\bf values} or {\bf strengths}
of the features.

\medskip

You might use the following set of 5 features ($K=5$) to describe
colored shapes against backgrounds.  The third column gives the
feature values for a dark striped circle $y$ against a light
background $x$:

\smallskip\noindent
\begin{tabular}{|c|l|c|} \hline
feature number & feature meaning   & feature value on this example \\ \hline
1              & $y$ is circular   &    1 \\
2              & $y$ is square     &    0 \\
3              & $y$ is striped    &    1 \\
4              & $y$ is a striped square & 0 \\
5              & how strongly $y$ contrasts with background $x$ & 2 \\ \hline
\end{tabular}
\smallskip

\noindent Thus, your description of this particular $(x,y)$ is given by
$\vec{f}(x,y)=(1,0,1,0,2) \in \Real^5$.  In English, you can say
that features 1, 3, and 5 {\bf fire} on this contextualized event
$(x,y)$, and that feature 5 fires twice (or fires with strength 2).

% That means that you chose to describe the contextualized event $(x,y)$
% as ``having'' features 1, 3, and 5 but not features 2 and 4.  
% 
% Furthermore, by defining $f_5(x,y)=2.0$, you chose to say that $(x,y)$
% has feature 5 especially strongly.  A common figure of speech is that
% feature 5 {\bf fires} especially strongly on $(x,y)$.

\medskip

It is slightly unusual that $f_5(x,y)=2.0$.  In principle a feature
value $f_k(x,y)$ can be any positive or negative real number.  But
most often your feature functions $f_k$ will be boolean functions, so
that $f_k(x,y)$ is always either 1 or 0, according to whether $(x,y)$
has feature $k$ or not.

% \subsection{Scores}


\subsection{The defining formula}\label{sec:formula}

The log-linear probability model $\ph$ computes a contextualized
event's probability from its features.\footnote{We are still using the
  $\ph$ notation because this is only a model that we are trying to
  fit to the data.  In most cases, we will never know whether the true
  probability distribution $p$ has this form.}  For example, given a
light background $x$, how likely is the shape $y$ to be a dark striped circle?

Specifically, the log-linear model defines
\begin{equation}\label{eqn:loglin}
  \ph(y \mid x) \defeq \frac{u(x,y)}{Z(x)}
\end{equation}
where the interesting part is the {\bf unnormalized probability}
defined by
\begin{align}\label{eqn:unnorm}
  u(x,y) &\defeq \exp \sum_{k=1}^K \left( \theta_k \cdot f_k(x,y) \right)  \\
         &= \exp \left(\vtheta \cdot \vec{f}(x,y)\right) > 0
\end{align}
Here $\vtheta \in \Real^K$ is a vector of feature {\bf weights}, which
are the adjustable parameters of the model.  You control these
with sliders in the interactive visualization.  

The unnormalized probabilities $u(x,y)$ are always positive (because
of the $\exp$).  Equation~(\ref{eqn:loglin}) has to scale them, in
order to get proper probabilities that sum to 1 (i.e., $\sum_y
\ph(y\mid x) = 1$ for each $x$).  As you can see in
(\ref{eqn:loglin}), this is a simple matter of dividing them by a {\bf
  normalizing constant}, namely
\begin{equation}
  Z(x) \defeq \sum_y u(x,y)
\end{equation}

Note that the conditional log-linear modeling framework has allowed us
to model $p(y \mid x)$ directly, rather than as a quotient $p(x,y) /
p(x)$.  We did not have to commit to any model of $p(x)$.

\subsection{Feature weights}

You can easily see that for any choice of $\vtheta$ and any $x$,
equation~(\ref{eqn:loglin}) gives a genuine probability distribution
over the events $y$ that could occur in context $x$.  You can also see
now why the model is ``log-linear''---for each context $x$, the
log-probability is a linear function of the feature vector:
\begin{equation}\label{eqn:logversion}
\log \ph(y \mid x) = \vtheta \cdot \vec{f}(x,y) - \log Z(x)
\end{equation}

% LEAVE THIS FOR THE ONLINE LESSONS
% This doesn't mean that our model $\ph$ is the {\em correct}
% distribution!  Does $\ph(y \mid x)$ equal the true probability
% $p(y \mid x)$?  There is no guarantee that $p$ can be accurately
% described at all using a log-linear model with these features,
% let alone with the specific weights $\vtheta$.

In our earlier example, setting the parameter $\theta_5 > 0$ makes
contrasting colors more common: the resulting $\ph(y \mid x)$ says
that dark shapes $y$ are probable when the background $x$ is light,
and vice-versa.  Alternatively, setting $\theta_5 < 0$ makes
contrasting colors less common.

In general, the linear coefficients $\vtheta$ in \eqref{eqn:logversion} are incredibly
important: the way you pick them will determine all the probabilities
in the model, and even the normalizing constants!  Fortunately, to
help you set $\vtheta$, you'll have a training set of events in their
contexts, $\{(x_1,y_1), (x_2,y_2), \ldots (x_N,y_N)\}$.  Then the
$\vtheta$ that maximizes
\begin{equation}\label{eqn:loglin-likelihood}
\prod_{i=1}^N \ph(y_i \mid x_i)
\end{equation}
does the best possible job of predicting these events in their
contexts.  More on this later.

\subsection{Unconditioned Models}\label{sec:unconditioned}

The initial lessons in the interactive visualization deal with the
simple case where there is no information about context.  So we can
omit $x$.  We now have a simple distribution over possible $y$ values,
\begin{align}
  \ph(y) &\defeq \frac{u(y)}{Z} \label{eqn:unconditioned} \\
  u(y) &\defeq \exp \left(\vtheta \cdot \vec{f}(y)\right) > 0 \\
  Z &\defeq \sum_y u(y)
\end{align}
Section~\ref{sec:concrete} of this handout shows what \eqref{eqn:unconditioned}
turns out to be for each $y \in {\cal Y}$ in the first two lessons.

\subsection{Features versus Attributes}

In practical settings, the outcome $y$ and the context $x$ are
characterized by their {\em attributes} (or {\em properties}).  Each
feature's value is then computed from one or more of these attributes.
$f_4$ and $f_5$ from section~\ref{sec:featfuncs} would be defined more
formally as
\begin{align}
f_4(x,y) & \defeq (\textsc{fill}(y)=\text{striped}) \wedge (\textsc{shape}(y)=\text{square})
             & \text{\small\it ``$y$ is a striped square''} \\
f_5(x,y) & \defeq |\textsc{brightness}(y)-\textsc{brightness}(x)|/100
             & \text{\small\it ``how strongly $y$ contrasts with background $x$''}
\end{align}
which refer to the \textsc{fill}, \textsc{shape}, and \textsc{brightness} attributes
of $y$ and/or $x$. 

You will sometimes hear people refer to individual attributes as the
``features'' or ``basic features'' of $x$ and $y$.  That's confusing.
I recommend that you use ``features'' only to refer to feature
functions (like $f_4$) or to the feature values that they return (like $f_4(x,y)$).

\subsection{Remark: Named Features and Efficient Dot Products}

In the notation above, we assigned numbers $1,2,\ldots,K$ to the
features.  This lets you represent the parameter vector $\vtheta$
(as well as each feature vector $\vec{f}(x,y)$) by an {\em array} of
$K$ real numbers.

However, it is often more convenient to refer to the features by names
instead of numbers.  The names can be strings or other objects that
describe the feature.  For example, in the table from
section~\ref{sec:featfuncs}, the first feature function might be
called $f_{\sf circle}$ instead of $f_1$, with corresponding weight
$\theta_{\textsf{circle}}$ instead of $\theta_1$.  Then the weight vector
$\vtheta$ can be implemented as a hash table instead of an array.  It
still maps the index $k$ to the weight $\theta_k$, but now $k$ is a
name instead of a number.

The dot product $\vtheta \cdot \vec{f}(x,y)$ is now defined by
$\sum_k \theta_k \cdot f_k(x,y)$ where $k$ ranges over the set of
feature names.  
In computing this sum, it is fine to skip features $k$ where $f_k(x,y)
= 0$.  In many cases, this is a big speedup because only a few of the
$K$ features will fire on a given contextualized event $(x,y)$.  For
example, $f_k(x,y)$ will be 1 for only one of the shapes $k \in \{{\sf
  circle}, {\sf square}, \ldots\}$, and 0 for all other shapes.  So the 
sum includes just one of the weights $\theta_{\textsf{circle}},
\theta_{\textsf{square}}, \ldots$.

In our example from section~\ref{sec:featfuncs}, $\vtheta \cdot \vec{f}(x,y)$ could be efficiently
computed as
\begin{algorithmic}
\State $s \gets 0$\Comment{initialize sum}
\State $s \gets s + \theta_{\textrm{shape}(y)}$\Comment{add weight of
  the single binary shape feature that fires on $y$: e.g.,
  $\theta_{\textsf{circle}}$}
\If{$y$ is striped}
  \State $s \gets s + \theta_{\textsf{striped}}$
  \If{$y$ is square}
    \State $s \gets s + \theta_{\textsf{striped square}}$
  \EndIf
\EndIf
\State $s \gets s + \theta_{\textsf{contrast}} \cdot f_{\textsf{contrast}}(x,y)
$\Comment{where $f_{\textsf{contrast}}(x,y) \equiv ||\textrm{color}(x)-\textrm{color}(y)||$
  is not binary}
\end{algorithmic}

\subsection{Remark: Other names for log-linear models}

Log-linear models $\ph(y)$ and conditional log-linear models
$\ph(y\mid x)$ are so prevalent that they have been rediscovered many
times!  So you will see lots of names for them.  The following are all
roughly equivalent: log-linear models, Gibbs distributions, undirected
graphical models, Markov Random Fields or Conditional Random Fields,
exponential models, and (regularized) maximum entropy models.  
Special cases include logistic regression and Boltzmann machines.

In all these cases, you define a probability by multiplying some
positive numbers together and dividing by a constant $Z$ so that your
probabilities will sum to 1.

\section{Finding the Parameters {\em [read with lesson 6]}}
  
We will try some examples in the interactive visualization where it's
tricky to adjust $\vtheta$ by hand.  So let's ask, how {\em should}
one adjust it?

\subsection{An intuition}

It turns out that maximizing equation (\ref{eqn:loglin-likelihood}) is
not too hard.  Suppose your training set has $N=100$ events of which
80 are circles (some solid, some striped).  But your current $\vtheta$
says that circles only happen 0.7 of the time, i.e.,
$$\ph(\textrm{striped circle}) + \ph(\textrm{solid circle}) = 0.7$$
so your model predicts that only 70 of the 100 events should be circles.

In that case, you should increase the weight of the ``circle''feature,
because the {\bf expected count} $70$ is smaller than the {\bf
  observed count} $80$ ($E < O$) and so you need to make circles more
likely in your model.  Similarly, if $E > O$ then you should decrease this weight.
Either way, by gradually adjusting this weight you can make $E=O$, so
that your model correctly predicts the observed number of circles.

As you've probably noticed, the feature weights do interact.  Once
you've made $E=O$ for the circle feature, trying to make $E=O$ for the
striped feature may mess up your first move, so that $E \neq O$ again
for the circle feature.  However, if you keep gradually adjusting all
of the features in this way, either simultaneously or one at a time,
then you will eventually reach a setting where $E=O$ for all features!

\subsection{The justification}

The setting where $E=O$ for all features is actually the
setting of $\vtheta$ that maximizes equation
(\ref{eqn:loglin-likelihood}), or equivalently, the setting that
maximizes the logarithm of (\ref{eqn:loglin-likelihood}):\footnote{It
  just happens that the logarithm is easier to work with
  mathematically, because of the $\exp$ in (\ref{eqn:unnorm}).  But
  why is it equivalent to maximize the logarithm?  Because $p_1 > p_2$
  if and only if $\log p_1 > \log p_2$.  That is, $\log p$ is
  an increasing function of $p$ (look at the graph of the $\log$ function).%
  % , which implies that
  % increasing $p$ will also increase $\log p$.  So if we are not yet at
  % a maximum of $p$, then we should be able to increase $p$ and
  % therefore we are still able to increase $\log p$.  Only when $\log
  % p$ reaches a maximum is $p$ maximized.
}
\begin{align}
  F(\vtheta) & \defeq \log \prod_{i=1}^N \ph(y_i \mid x_i) = \sum_{i=1}^N \log \ph(y_i \mid x_i) 
\label{eqn:loglin-log-likelihood}
\end{align}

\subsection{Formula for the gradient}

You can see this by computing the partial derivatives of $F(\vtheta)$ 
with respect to the $k$ weights:

\begin{align}
  \diffk F(\vtheta) 
  &= \sum_{i=1}^N \diffk \log \ph(y_i \mid x_i)  \\ %   - \left( \theta_k / \sigma^2 \right) 
  &= \sum_{i=1}^N \diffk \log u(y_i \mid x_i) - \diffk \log Z(x_i) \\
  & \hspace{1in}\vdots \nonumber \\ 
  &= \underbrace{\left( \sum_{i=1}^N f_k(x_i,y_i) \right)}_{\textit{observed count $O$ of feature $k$}}
  - \underbrace{\left( \sum_{i=1}^N \sum_y \ph(y\mid x_i) f_k(x_i,y) \right)}_{\textit{expected count $E$ of feature $k$}} \label{eqn:obsexp}
\end{align}

At the max of $F(\vtheta)$, this derivative must be 0 for each
$\theta_k$, and hence $E=O$ for each feature $k$.  

{\bf Clarification:} So far in the online toy, you have only been
modeling $\ph(y)$.  However, soon you'll try $\ph(y\mid x)$ as
defined in section~\ref{sec:formula}.  What does ``expected count'' mean in this
setting?  (\ref{eqn:obsexp}) defined it as $\sum_{i=1}^N \sum_y
\ph(y\mid x_i) f_k(x_i,y)$.  This measures how many times $f_k$ would
have fired on average if we had still observed exactly the same
training contexts $x_1,\ldots x_n$, but if the corresponding training events
$y_1,\ldots y_n$ were respectively replaced by random draws from the
conditional distributions $\ph(y \mid x_i)$.

\subsection{Algorithm for maximizing the log-probability}

It can be shown that $F$ is a convex function, so it is like a simple
hill with only one local maximum.  There are many reasonable ways to
climb to the top of this hill.  One option is {\bf gradient ascent}, a
simple function maximization algorithm that increases $\theta_k$ at a
rate proportional to $\diffk F(\vtheta)=O-E$, for all $k$ in parallel.
These rates of increase are positive or negative for $E < O$ or $E >
O$ respectively.  As $\vtheta$ is adjusted, these rates will also
gradually change, eventually reaching 0 at the top of the hill.

\subsection{Details for the curious}

We got to (\ref{eqn:obsexp}) above by applying basic rules
of differentiation:
\begin{align}
  \diffk \log u(y \mid x) 
  &= \diffk \left( \sum_{k=1}^K \theta_k \cdot f_k(x,y) \right) \\
  & = \diffk \left( \theta_k \cdot f_k(x,y) + \textrm{constant not involving $\theta_k$} \right) = f_k(x,y)\\
%
  \diffk \log Z(x) &= \frac{1}{Z(x)} \diffk Z(x) 
  = \frac{1}{Z(x)} \sum_y \diffk u(x,y)  \\
  &= \frac{1}{Z(x)} \sum_y \diffk \exp (\vtheta \cdot \vec{f}(x,y))\\
  &= \frac{1}{Z(x)} \sum_y \left( \exp (\vtheta \cdot \vec{f}(x,y)) \right) 
  \left( \diffk (\vtheta \cdot \vec{f}(x,y)) \right) \\
  &= \frac{1}{Z(x)} \sum_y u(x,y) \cdot f_k(x,y) \\
  &= \sum_y \ph(y\mid x) \cdot f_k(x,y)
\end{align}

\section{Regularizing the Parameter Estimates {\em [read with lesson 8]}}

Problems sometimes arise when we simply maximize the log-probability.
Why does this happen?  Can we fix it?

\subsection{Overfitting, underfitting, and the bias/variance tradeoff} 

You saw in lesson 8 that maximizing $F(\vtheta)$ might actually be a
bad idea.  The resulting estimate of $\vtheta$ makes the {\em
  training} data very probable, but it might not generalize well to
{\em test} data.  That is called {\bf overfitting} the training data.

In fact, there is a close connection to the bad unsmoothed estimates
of section~\ref{sec:mle}, which also overfitted.  Both cases use {\bf
  maximum-likelihood estimates}---that is, they model the training
data as well as they can, using the available
parameters.\footnote{Technically, the {\bf likelihood} of the
  parameter vector $\vtheta$ is defined by
  (\ref{eqn:loglin-likelihood}), which says how probable the training
  data would be if we used {\em that} $\vtheta$.  We have been finding
  the {\bf maximum-likelihood} estimate of $\vtheta$ by maximizing the
  likelihood (\ref{eqn:loglin-likelihood}) or equivalently the
  log-likelihood (\ref{eqn:loglin-log-likelihood}).}

\subsection{How many parameters?}

What are the available parameters?  In section~\ref{sec:mle} we were
able to choose the probabilities individually, so we could model the
training data perfectly (a bad idea!).  But now we can only manipulate
the probabilities indirectly through the $\vtheta$ parameters of our
log-linear model.

If the log-linear model has only a few parameters, that may save us
from overfitting---we will be unable to fit the training data too
closely, which forces us to generalize.  You saw this in lesson 3 of
the interactive visualization.  On the other hand, if the log-linear
model has enough features, as in lesson 4, then you can again fit the
training data perfectly.  Maximizing (\ref{eqn:loglin-likelihood})
will then get you exactly the same bad probabilities as in
section~\ref{sec:mle}.

So perhaps we should simply build a simple log-linear model with few
parameters.  But wait \ldots do we really want to break our good model
and get rid of potentially useful features??  Then we might have a
problem of {\bf underfitting}---not matching the training data closely
enough.

In general, it's reasonable for the number of parameters to increase
with the size of the training data.  We see exactly the same issue
when choosing between bigram and trigram models.  We should be able to
match true English better with a trigram model (or better yet, a
50-gram model), but we may not have enough training data to estimate
trigram probabilities accurately.  In that case we fall back to a
bigram model, which reduces variance (to avoid overfitting) but
unfortunately increases bias (leading to underfitting).

\subsection{The regularization idea}

We'd prefer not to make a strict choice between pure bigram and
pure trigram models.  The training corpus may have plentiful evidence
about {\em some} trigrams while lacking evidence about others.  This
line of thinking leads to backoff smoothing---the topic of the next
assignment.

How does this idea apply to log-linear models?  In general, we'd like
to include a whole lot of potentially useful features in our model,
but use only the ones that we can estimate accurately on our
particular training data.  Statisticians call this idea {\bf
  regularization}.  In the log-linear setting, a feature with weight 0
has no effect on the model, and a feature with low weight has little
effect.  So the regularization principle is ``All weights should stay
close to 0, except where really needed to model the data.''

So instead of maximizing the log-likelihood 
(\ref{eqn:loglin-log-likelihood}), let's maximize the
{\bf regularized log-likelihood}, 
\begin{equation}\label{eqn:loglin-regularized}
  F(\vtheta) = \left( \sum_{i=1}^N \log \ph(y_i \mid x_i) \right) - C\cdot R(\vtheta)
\end{equation}
where $R(\vtheta)$ is a penalty for weights that get far away from 0.
A parameter $C \geq 0$, which can be tuned via cross-validation, will control the size of this penalty and
hence the strength of regularization. 

\subsection{Types of regularization} 

A common choice is called $\ell_2$ (or $L_2$) regularization.  Define
$R(\vtheta) = ||\vtheta||^2 = \sum_i \theta_i^2$.  Then
\begin{equation}
  \diffk F(\vtheta) = \textrm{observed count of $k$} -
  \textrm{expected count of $k$} \underbrace{\mbox{}-2C \cdot
    \theta_k}_{\textit{new term}}
\end{equation}
Because of the new term, gradient ascent will tend to push $\theta_k$
toward 0 (increase it if it s negative, decrease it if it's positive).
For large values of $C$, the optimal $\theta_k$ will be usually be
close to 0.  {\em However,} if feature $k$ is sufficiently common in the
training data, then $\diffk F(\vtheta)$ will be dominated by the
counts, and the regularization term will have comparatively little
influence.  In this case, $\theta_k$ will be able to pull farther away
from 0 in order to model the observations---which is exactly what we
wanted to happen for well-observed features.  

Another common choice is $\ell_1$ (or $L_1$) regularization.  Here 
$R(\vtheta) = \sum_i |\theta_i|$.  Then
\begin{equation}
  \diffk F(\vtheta) = \textrm{observed count of $k$} -
  \textrm{expected count of $k$} \underbrace{\mbox{}-C\cdot \textrm{sign}(\theta_k)}_{\textit{new term}}
\end{equation}
This has a similar effect, but with the nice property that the optimal
$\theta_k$ often equals 0 exactly.  (Can you figure out why?  Hint:
use the formula above to graph how $F(\vtheta)$ varies with $\theta_k$ when
$\theta_k$ is close to 0.  What happens when $C$ is large enough?)

Notice that for any $C > 0$, regularization will always prevent
$\theta_k$ from zooming off to $\infty$ or $-\infty$ (because that
would make $R(\vtheta)$ infinitely bad).  Happily, this prevents us
estimating $\ph(y\mid x)=0$ as happened in lesson 8.

\subsection{Remark: A Bayesian interpretation}

One way to interpret this regularization is in terms of Bayes'
Theorem.  Suppose we want to find the most likely value of $\vtheta$,
given the training data.  That is, instead of choosing $\vtheta$ to
maximize $p(\textrm{data} \mid \vtheta)$ (the {\bf maximum likelihood}
principle), we'll more sensibly choose it to maximize $p(\vtheta \mid
\textrm{data})$ (the {\bf maximum a posteriori (MAP)} principle).  By
Bayes' Theorem, $p(\vtheta \mid \textrm{data})$ is proportional to
\begin{equation}
\underbrace{p(\textrm{data} \mid \vtheta)}_{\textit{likelihood}} \cdot \underbrace{p(\vtheta)}_{\textit{prior}} 
\end{equation}
And maximizing the log of this is precisely the same as maximizing
(\ref{eqn:loglin-regularized}), provided that we define $p(\vtheta)$
to be proportional to $\exp -R(\vtheta)$.  In other words, our prior
distribution says that if $R(\vtheta)$ is large then $\vtheta$ has low
probability.  From this point of view, $\ell_2$ regularization
corresponds to a Gaussian prior,\footnote{Since the exp of a quadratic
  function is a Gaussian function.  This looks like a bell curve in
  $k$ dimensions, with most of the probability near the
  origin: \includegraphics[scale=0.4]{gaussian} This choice explicitly defines the prior
  probability of $\vtheta$ to be proportional to $\exp -\frac{1}{2\sigma^2} \sum_k
  \theta_k^2$, where $\sigma^2 (= 1/2C)$ is called the {\em variance} of the
  Gaussian.  Clearly, this is just a convenient assumption about
  where the true value of $\vtheta$ is most likely to fall.%
  % (Similarly, add-$\lambda$ smoothing (even with backoff) can be
  % derived from a certain prior assumption about the true probabilities
  % $p(z \mid xy)$.  Ask me if you're curious.)
}  and $\ell_1$
regularization corresponds to an exponential prior.  Both of these say
that {\em a priori}, we expect each $\theta_k$ to be close to 0.

\end{document}
